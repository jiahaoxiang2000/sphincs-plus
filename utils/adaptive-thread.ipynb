{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## theory analysis \n",
                "\n",
                "First axiom: the parallelism number $t_i$ is the max CUDA thread number, the performance is the best for the function $g_i$. then we know the signature process including the public key generation, signing, and verifying. Those processes need serial run the function $g_i$. Not the optimal approach is use the parallelism number $t_i$ to the max CUDA thread number.\n",
                "The performance is not the best for the function $g_i$.\n",
                "\n",
                "Those have the Adaptive Thread function (AT) $AT:G\\rightarrow T$, which maps each function $g_i \\in G$ to its optimal thread count $t_i \\in T$.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To accurately define this mapping, we can approach it through empirical performance modeling:\n",
                "\n",
                "1. **Empirical Performance Model**: For each function $g_i$, we model execution time as:\n",
                "\n",
                "   $T(g_i, t) = \\alpha_i + \\frac{\\beta_i}{t} + \\gamma_i \\cdot t$\n",
                "\n",
                "   Where:\n",
                "\n",
                "   - $\\alpha_i$ represents fixed overhead cost\n",
                "   - $\\frac{\\beta_i}{t}$ captures the parallel speedup component\n",
                "   - $\\gamma_i \\cdot t$ models thread management overhead\n",
                "   - $t$ is the thread count\n",
                "\n",
                "2. **Parameter Estimation**: We can estimate $\\alpha_i$, $\\beta_i$, and $\\gamma_i$ by running benchmarks with varying thread counts and performing regression analysis.\n",
                "\n",
                "3. **Optimal Thread Count**: The optimal thread count $t_i^*$ for function $g_i$ can be derived by finding the minimum of $T(g_i, t)$:\n",
                "\n",
                "   $t_i^* = \\sqrt{\\frac{\\beta_i}{\\gamma_i}}$\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "4. **Implementation Strategy**: For practical implementation, we can:\n",
                "   - Perform offline profiling for each key function\n",
                "   - Build a lookup table mapping functions to their optimal thread counts\n",
                "   - Apply dynamic thread adjustment based on runtime conditions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "here we have one kernel function $g_i$, the blocks*threads is the $t_i$, the time is the $T(g_i, t)$, the per op is the $\\frac{T(g_i, t)}{n}$, where $n$ is the number of operations of kernel function.\n",
                "\n",
                "```csv\n",
                "blocks, threads, time(ms), per op(ms)\n",
                "32, 64, 310.03, 0.0095\n",
                "64, 64, 155.41, 0.0047\n",
                "96, 64, 118.89, 0.0036\n",
                "128, 64, 90.56, 0.0028\n",
                "160, 64, 102.16, 0.0031\n",
                "192, 64, 85.41, 0.0026\n",
                "224, 64, 87.52, 0.0027\n",
                "```\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "SageMath 10.4",
            "language": "sage",
            "name": "sagemath"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
